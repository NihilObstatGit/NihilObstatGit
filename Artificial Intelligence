Module 21
Module Introduction
1.	“Artificial intelligence and machine learning, as a dominant discipline within AI, is an amazing tool. In and of itself, it’s not good or bad. It’s not a magic solution. It isn’t the core of problems in the world.” – Vivienne Ming, Executive Chair and Co-Founder, Socos Labs
a.	Can machines think on their own? Can machines mimic the human brain, its behavior, and qualities? People have grappled with these questions for centuries. This fascination about machines impersonating and even transcending humans was amplified with blockbuster Hollywood science fiction hits such as Metropolis in 1927 and the Wizard of Oz in 1939 (Anyoha, 2017). By the 1950s, scientists, mathematicians, and philosophers all began to research and engage to see how a human-like robot could be possible (BBC, n.d.). 
b.	Today the mass adoption of technology has led to vast amounts of data that have outpaced humans’ ability to understand, evaluate, and use to make complex decisions. Fortunately, one of the critical components of achieving AI and allowing it to thrive is the availability of large amounts of data. The other two components required for AI are robust computational systems and algorithms (code) (NetApp, n.d.). 
i.	Autonomous vehicles
ii.	Email spam filters
iii.	Facial recognition
iv.	Internet of Things (IoT)
v.	Marketing and advertising
vi.	Navigation apps
vii.	Recommendations on media streaming services
viii.	Rideshare apps
ix.	Smart home assistants
x.	Social media
c.	There is no question that AI is here to stay with a bright future ahead of it. For this reason, it is necessary to become familiarized and comfortable with the technology. Furthermore, from a cybersecurity perspective, besides understanding the typical applications of AI, you also need to know how attackers can abuse AI to advance their malicious objectives. 
2.	Module Overview:
a.	Discuss artificial intelligence and machine learning.
i.	Discuss how ML models are trained and deployed and view some examples.
b.	Identify malware detection, a type of attack on malware detectors, and how to make models more resilient to this type of attack. 
c.	Identify how an algorithm inspired by natural evolution can model the co-evolution of attacks and defenses in cybersecurity. 
Artificial Intelligence
1.	The term AI was coined in 1955 by John McCarthy, an American computer and cognitive scientist. In 1956, McCarthy and others organized the “Dartmouth Summer Research Project on Artificial Intelligence” conference. This conference seeded the collaborative cross-disciplined effort that eventually blossomed into data science, ML, deep learning, and other related sub-fields.
a.	Unfortunately, most of the early efforts in AI failed to become reality. First, the reason behind this was mainly due to the fact that the computers of 1949 could not store commands. Second, the first computers were highly costly. For example, leasing a computer in the early 1950s ran as high as $200,000 a month (Anyoha, 2017). However, over the coming decades, the technology in computers significantly improved, and they also became more efficient and affordable, which led to much broader adoption. 
2.	One definition of AI is the “simulation of human intelligence in machines that are programmed to think like humans and mimic their actions” (Frankenfield, 2021). An essential element of this definition worth exploring further is “human intelligence” because it is broad and complex. For example, as a human, you see with your eyes and instantly process the information received. In addition, you observe and assess your physical environments and determine the safest way to navigate around them. Furthermore, besides visual abilities, humans can understand spoken languages, sense emotions and, through thoughts, can simulate events in the past, present, and future.
a.	While the above examples of human intelligence are not exhaustive, they demonstrate the variety and depth of the human intellect and how challenging it is to mimic it all in a machine. AI is a broad branch of computer science that includes many different sub-branches, including ML, robotics, computer vision, speech recognition, natural language processing, reinforcement learning, and many others. 
3.	Deep models of code automate challenging and manual tasks of software engineering (such as testing) and cybersecurity such as penetration detection).
4.	Genetic Programming:
a.	Uses evolutionary algorithms inspired by neo-Darwinian evolution that works with populations, survival of the fittest, and genetic variation to automatically model data and synthesize code. 
5.	Artificial Intelligence:
a.	A quest to make computers as intelligent as humans and make them intelligent in ways that humans are not. 
6.	Human Intelligence: Vision and Language
a.	Language:
i.	Has the capacity to share complex thoughts via language.
ii.	Is a powerful code to exchange information, find culture, and convey deep personal feelings. 
b.	Vision:
i.	Visual processing involves object detection and recognition, high-level scenes, and moving images.
c.	Goal of AI: To understand what knowledge representations and mental computations underly language and visual processing. 
i.	Learn to understand what you see.
ii.	Learn to communicate with language. 
Machine Learning
1.	Humans are born with limited knowledge and, therefore, initially can do very little by themselves. However, by being exposed to different situations and experiences, humans learn and become more capable. Likewise, if you want a computer to do a task that it was not previously programmed to do, it needs to follow a similar learning process. For a computer to experience different situations that it can learn from, it needs lots of historical data. Large datasets allow a computer to identify patterns and develop a model capable of processing similar situations in the future more accurately. This concept essentially forms the fundamentals of ML. 
2.	Machine Learning:
a.	Proceeds differently from how humans learn.
b.	Is data-driven and relies on historical data rather than new experiences or help. 
3.	Machine Learning Model:
a.	Uses historical examples to extrapolate a model ready to handle similar examples in the future. 
b.	Generalizes from the historical examples that you collect and label. 
c.	For example, about 97% of malicious Gmail attachments that are blocked each day are different from the day before. There are variations that are detected by a model that is trained to generalize and is retrained regularly. 
i.	The Gmail Malicious Attachment Detector model makes its prediction by non-linearly combining a set of features extracted from its input. 
ii.	To be able to predict accurately, you train the ML model, using the features you extract from historical examples to minimize its predictive error on the training examples’ labels. 
4.	Machine Learning Demonstration Models:
a.	It combines two kinds of deep learning models:
i.	Convolutional neural networks that process images. 
ii.	Bi-directional recurrent neural networks that process language. 
AI in 60 Minutes
Within the past couple of years, I would say AI has affected my way of life drastically. From medical sciences, deep learning algorithms that have shaped entire elections (Cambridge Analytica for example), artificial narrow intelligence, and various artificial general intelligence is a fascinating concept that makes me hopeful for the future. I subscribe and listen to Deepmind and whenever they push out new podcasts every other year it makes me genuinely hopeful that AI will revolutionize the way we all work and interact with the world. Playing the game, I already understand by the second question that it was trying to get me to understand machine learning. Having no concept of what a Yoda, Fluffer, and Skizzle is and what a Digikon is on top of that I knew it was going to have to do with something about learning and the fact that we can go back and try again practically encourages people to learn through repetition. By the fourth question I understood the basic pattern of what Yodas (rhombus-like shapes), fluffers (red-outlined shapes), and skizzles (triangle-like shapes) were. The fundamental concept of an AI is a computer system doing things that normally need human intelligence. They typically require things like vision and/or language to accomplish those things and lots and lots of data for AI to discriminate against. What is an isn’t a tiger perse, and what qualifies a tiger to be a tiger and how to differentiate it from any like things or how tone, culture, and objectives in conversation can be deployed to communicate with users. The AI listed above is what is currently in existence and is considered artificial narrow intelligence, or AI directed toward a single task. ANI is all about training machines and systems to perform certain tasks; alongside learn from that task and adapt to its behavior as a result of a learning process and can help humans perform tasks that may be menial or lead to a life less fulfilled. Artificial general intelligence is generally the concept of a sentient AI and can incorporate both vision, language, and its own goals – examples include things like the Terminator or WALL-E. AGI in a sense that humans are actively trying to achieve in the real world generally mean an AI that is good at everything a human can do, they can transfer learned skills to new areas (incorporating a past, present, and future vision to understand consequences of actions), and unfortunately given our current state of affairs in computing does not exist, but may some day exist as computers become more efficient and more powerful. In Me, myself and AI in the DeepMind Podcast Hannah Fry looks at “deepfake” technology and how it can be used to improve weather forecasting and how DeepMind researchers are collaborating with Liverpool Football Club to me is the most surprising thing that came out this year. It’s similar to Google’s AI assistant that can call and use a user’s voice and naturalize their voice to call and schedule appointments, which is scary to think about from a security standpoint; however, the medical implications of being able to assist people with ALS regain their voice is extremely satisfying to know as a human that we are living in a time where AI is going to greatly assist us in living in an age of abundance. Furthermore, AlphaFold recently revealed the structures of all the proteins known to science, expanding the AlphaFold DB by over 200x. “AlphaFold has demonstrated benefits in fields like strengthening our capacity to combat plastic pollution, gaining an understanding of Parkinson’s disease, improving honeybee health, comprehending how ice forms, combating neglected diseases like Chagas disease and Leishmaniasis, and investigating human evolution.” (Anwar, 2022). I truly believe, we are being catapulted into a future that can unlock all of what humanity has to offer with the help of AI and that this is the next big step towards evolving alongside artificial intelligence. 
References: 
Anwar, D. T. (2022, July 30). Deepmind's alphafold revealed the structures of all the proteins known to science, expanding the alphafold DB by over 200x. CBIRT. Retrieved July 31, 2022, from https://cbirt.net/deepminds-alphafold-revealed-the-structures-of-all-the-proteins-known-to-science-expanding-the-alphafold-db-by-over-200x/ 
Explore and Experiment
Netflix’s machine learning platform was the most interesting to me, because it brings to mind the machine learning algorithms that are utilized with DeepMind’s Alpha artificial intelligence. These can include AlphaGo, AlphaStar, AlphaChess, and AlphaFold. These are all artificial narrow intelligence that utilize machine learning algorithms to better understand the task it is set to complete and become the best at. GPT-3 was actually on one of DeepMind’s podcasts and The Pudding perfectly captures the power that GPT-3 has when it comes to artificial intelligence’s capacity to also be emotionally intelligent while also assisting users find creative endings. It’s almost as if AI can not only learn general tasks that are also menial but are also capable of bringing a creative twist to the table and its capacity to collaborate with users can change our view of artificial intelligence to become more collaborative. Instead of wondering what AI can do for us, it makes us think what can we accomplish with AI? This is the way I speak about AI when I talk about it with others and should be the way society views AI in general. The things we can accomplish alongside artificial intelligence is going to bring about the next step in human evolution, but if we use AI as a means to an end, then we can quickly find that AI will take us to that end a lot quicker and we must be careful about where and what we use AI for. 
Intelligent and Adaptive Adversaries
1.	While AI is tremendously helpful in cyber defense by using ML to analyze large quantities of data to identify abnormal behavior, unfortunately, the same power of AI is used by attackers to advance their objectives. By leveraging the abilities of AI, attackers can amplify their attacks, which result in more unpredictable, agile, and clandestine attacks that devastate organizations that are not prepared. 
a.	The incredible ability of AI to support human labor across industries means that it also can be utilized to automate large-scale offensive cyber-attacks. The result is attackers launching strikes at unprecedented levels of sophistication while employing even less time and effort than before. In addition to attackers leveraging AI to launch external attacks, another serious concern is that attackers could manipulate AI itself. According to Gartner, 37% of organizations surveyed in 2019 now use AI in the workplace. The number of enterprises using AI in business grew by 270% between 2015 and 2019 (Jovanovic, 2021). These statistics demonstrate how organizations are becoming more accustomed to and dependent on using AI to support their business. Therefore, these AI systems become high-value targets for attackers. An example of such an attack is AI poisoning. This is where attackers attempt to manipulate the datasets used to train the AI system or make subtle changes to the models to align them with their nefarious objectives. Furthermore, suppose the AI system that the attacker manipulates is designed to prevent or detect cyber intrusions or malware. In that case, it could result in an attacker successfully evading detection or raising any suspicion. 
2.	In environments, we can talk about one side being favored (good side) and the other being out of favor (bad side). When these adversaries clash in terms of outcomes what is bad for the good side is reciprocally good for the bad side. 
a.	Adversarial settings:
i.	Are located in cyberspace.
ii.	Abounds with intelligent adaptive adversaries. 
3.	Attacks:
a.	Example: Botnet attack
i.	Goal: To deny service to some target.
ii.	Strategy: To compromise many different servers on the internet and then later direct a coordinated attack that overwhelms the resources of the target so that it cannot legitimately serve its business needs. 
b.	Bot operators now sell their services, and countermeasures have been quickly adapted to fight the attacks. 
4.	Report by a leading Content Delivery Network Provider:
a.	The x-axis is attack and defense sophistication
b.	The Y-axis is time
c.	Single IP, Multiple IPs, low request rate, randomized user agent, browser impersonation, session replay, full cookie support, javascript support, browser fingerprint spoofing and recorded human behavior. 
d.	IP blocking and rate limiting are at the low end of evolving bot landscape detection
e.	HTTP anomaly detection – full cookie support
f.	Browser fingerprinting – Javascript support
g.	Recorded human behavior – User behavior analysis
Offensive AI
1.	“The use of smart tools to automate the attack process was inevitable. For instance, if a human attacker has to spend a lot of time trying different routes into a target network, adapting after each attempt, and deciding what to try next, why not teach that process a piece of software?” – Bryan Betts, principal analyst at Freeform Dynamics 
a.	According to the World Economic Forum, adversarial AI (also known as offensive AI) is the “malicious development and use of advanced digital technology and systems that have intellectual processes typically associated with human behavior. These include the ability to learn from past experiences, and to reason or discover meaning from complex data” (Dixon, 2018). 
b.	Furthermore, a 2020 Forrester study found that “close to 80% of cybersecurity decision makers anticipate offensive AI to increase the scale and speed of attacks. In addition to their quickness, 66% also expect offensive AI to conduct attacks that no human could conceive of” (Forrester, 2020). 
c.	Unfortunately, the use of AI has already impacted the threat landscape of today. Some examples of this are:
i.	Generating highly customized malware that evades traditional safeguards. 
ii.	Discovering vulnerabilities and identifying the attack path of least resistance. 
iii.	Unleashing sophisticated campaigns across various online platforms to spread disinformation and deep fakes. 
Offensive AI
Offensive AI is essentially automated technology used to facilitate the enumeration and exploitation of commonly used attack vectors in the cyberthreat landscape. It’s because of the speed and use of AI that allows for a wider range of attack vectors that organizations must attune to and become aware of to gain visibility in the cyber threat landscape. Having more points of entry and to allow it to be automated makes hacking a lot easier, because AI can do it for us and report back with results in a matter of time, while more work on enumeration can be done. This effectively cuts the work of attackers almost completely and essentially allows for more attack vectors in the cyber threat landscape. Cybersecurity is identified as a fertile area for AI-enabled vulnerabilities which disrupt the flow of information and cause any organizations to suffer previously unbeknownst attacks, because it effects the integrity of the organization in general. The efficient manipulation of information technology is the goal of Offensive AI and unfortunately with advancements of AI also brings to light the darker part of humanity that seeks to disinform and cause ethical and legal concerns with the generation of disinformation in election campaigns utilizing AI and enabling AI to be utilized against publicly available information that can be harvested on the internet. In general AI is suited for information warfare and cybersecurity applications. Augmenting IoT-targeting malware like Mirai with intelligence can vastly improve the strategic potential of malware. Stuxnet is particularly an illustrative example of how decisive, advanced, and strategically targeted malware can be. Data diet vulnerability is another attack surface found in much of current autonomous learning systems and a good AI is only as good as the information or data it is provided. Disinformation schemes can be used against AI and can essentially cause AI to become a double agent working and employed by the organization, but also working with an adversary. For example, AI surveillance systems or cybersecurity for national security opens a new attack vector based on this data diet vulnerability. Recent research has shown that the viability of “training set poisoning” for machine-learning-based malware detection systems and can be particularly vulnerable to organizations that put complete confidence in their AI without sufficient data analysis testing. 
References:
The risks of artificial intelligence to security and ... - Rand Corporation. (n.d.). Retrieved August 2, 2022, from https://www.rand.org/content/dam/rand/pubs/perspectives/PE200/PE237/RAND_PE237.pdf 
Malware
1.	Today’s malware has taken a much more sophisticated profile by using advanced approaches to evade detection and gain persistence on the compromised target. Part of the reason is that , unlike the malware of the 1970s, today, malware is supported by elaborate groups of organized criminals and state-sponsored actors that have access to extensive resources to conduct research and development. One of the common ways to detect the malware used by traditional anti-malware products is a signatures database. When these products encounter anew file or scan a system, they cross-reference the file’s attributes against the database for matches. And, while this detection approach is still effective against commodity malware, it is generally no match against today’s advanced malware. 
2.	The goal of malware is to illicitly enter a computer system and disrupt it or steal from it. The attacker injects code in a file that looks fine to the receiver. 
a.	This code is harmful and when it executes through some trick on the user or in the program logic, malicious activity occurs on the computer system. 
3.	Malware has become an umbrella term for a virus, worm, Trojan Horse, spyware, and etc.
4.	Ways for malware to enter a computer:
a.	The user installed something they really should not have from an untrustworthy source.
b.	The user did not pay attention when installing a reputable application that bundled optional crapware. 
c.	The computer is already affected, and the malware is installed by prior malware that sits on the computer.
d.	A high-quality antivirus or anti-spyware application has not been used during the download. 
5.	Examples of Spyware:
a.	The user is lured to:
i.	Click a URL, whereupon new files themselves do damage. 
ii.	Work with other malware that was downloaded earlier but now invades the user’s computer. 
6.	How is Malware Created?
a.	Source code
b.	Compile
c.	Object file
d.	Runtime library files linking
e.	Executable file
Malware Detection
1.	“In an age of dynamic malware obfuscation through operations such as mutating hash, a hyper-evolving threat landscape, and technologically next generation adversaries, offensive campaigns have an overwhelming advantage over defensive strategies.” – James Scott, Senior Fellow, Institute for Critical Infrastructure Technology
a.	To combat the dynamic nature of today’s malware, malware detection tools and techniques have also evolved. Previously, malware detection was mainly dependent on models of bad behavior or characteristics through creating effective signatures. When such a signature flagged a file, there was a high probability that it was malicious. However, as the volume and sophistication of malware increased, attackers could easily evade detections that purely relied on signatures. Therefore, today, many malware detection products leverage ML to effectively detect known and unknown malware related threats. Additionally, when the ML model for detecting malware is properly trained and implemented, it can detect malicious software without needing preconfigured signatures. 
2.	Training the Detector:
a.	Step 1:
i.	If the detector needs to work on applications running on mobile devices, the examples (data) will be Portable Executables (PEs).
ii.	Collect benign and malicious examples (data).
iii.	Detector has to examine and classify whether the data is malicious or not. 
iv.	Distill features.
v.	PEs can be compiled to run on multiple brands of devices by linking to different libraries, each one specific to the device brand. 
b.	Step 2:
i.	Divide the labeled examples, with their features into two subsets. 
ii.	Subset one: Will be used with its label for training the model to correctly detect a PE as being malicious or benign. 
iii.	Subset two: Will be used for testing to gauge the accuracy of the trained model by testing its examples with labels hidden. 
iv.	The model is further trained with these examples. 
v.	Training:
1.	Starts with random model parameters which imply the model has poor accuracy.
2.	Involves: 
a.	Presenting the examples.
b.	Observing errors computationally.
c.	Tuning the parameters of the model in a direction away from the error. 
AI in Cybersecurity
1.	“In 2010, McAfee thought it impressive that it was discovering a new specimen of malware every fifteen minutes. In 2013, it was discovering one every single second!” – PW Singer, Cybersecurity and Cyberwar: What Everyone needs to Know
2.	In basic terms, malware can be described as any malicious program designed to wreak havoc on or damage a computer system. The most common types of malwares include rootkit, botnet, worm, spyware, info stealer, and trojan horses. Furthermore, malware is a perpetually evolving threat with a constant oscillation between cyber defenders and attackers.
a.	The malware employed by today's attackers is more sophisticated and being created at an unprecedented speed. According to the 2020 G Data Threat Analysis (Links to an external site.), "cyber criminals published 76 new versions of malware every minute." (G Data, 2020)
b.	In this module, you learned how AI and ML are used to tackle the complicated and serious problem of detecting malware. In this assignment, you will study a whitepaper, The Rise of Machine Learning in Cybersecurity (Links to an external site.) by CrowdStrike, about how a leading cybersecurity organization has incorporated ML in its malware detection product. In addition, you will explore the limitation of using ML in detecting malware and the options for compensating for those limitations.
i.	The whitepaper describes the three general categories of ML. Can you define each of the three categories and their examples? Which category does the vendor use in malware detection?
ii.	What are the dimensions in ML, and how many can be used?
iii.	What does the term "generalization" mean in ML? How does it help in detecting previously unseen malware?
iv.	Why is ML such an effective tool against advanced unknown malware?
v.	How do ML technologies differ, and what factors can increase the accuracy and effectiveness of the ML model?
vi.	What are the limitations of ML in detecting malware? How can an attacker defeat even a highly effective malware ML engine? Furthermore, what is the role of intrusions that do not use malware?
vii.	What is the recommended solution for compensating for the limitations of ML in malware detection?
viii.	What are indicators of attack (IOAs), and how effective are they?
ix.	Besides detecting sophisticated malware, what other cybersecurity areas can components of AI help? For example, in your opinion, how can components of AI improve network-based intrusion detection systems, phishing analysis, and security operations centers?
There are three general categories of machine learning (ML) that are described in Crowdstrike’s Whitepaper entitled “The Rise of Machine Learning in Cybersecurity”. These three categories include: supervised, unsupervised, and reinforcement learning. In supervised learning, the machine is trained on data that is labelled so the machine understands what the data represents. For example, it discriminates against a data point for what it is and what it isn’t and matches it against input and knows what answers are expected (output). Based on this type of training, supervised learning has applications in disease diagnostics, or speech recognition. In unsupervised learning, the machine is trained using data that doesn’t have labels or in other words does not know what the data represents nor what answers are expected, and the machine will essentially have to figure out on its own the patterns and structure of the unlabeled input and discover the expected output. Machines that classify movie genres in Netflix is an example of unsupervised learning. Finally, reinforcement learning allows machines to interact with its environment to achieve a certain goal. It is also trained using unlabeled data like in unsupervised learning. However, the machine receives feedback on the outcome unlike in unsupervised learning. For example, a machine that learns to play a game based on some type of reinforcement model – positive feedback for a win and negative feedback for a loss and from the actions it takes, it will, over time, determine by itself the best strategy to win the game. Each victory thus, will reinforce the validity of specific actions that it is trained on and is emerging in robotics for manufacturing namely. 
ML can actually classify thousands of features or dimensions. Dimensions are essentially examples of things that a machine uses to learn. For example, for the machine to have a concept of data it needs to know how to extract the right features and on top of that run a classifier that balances true positives and false positives. For example, a graph with colored dots indicating male and female and their age on a two dimensional graph may look like they are mirroring each other and a pattern like that would generally need to be taught to a machine to understand what the information means. Running a machine learning classifier or classification algorithm essentially makes that distinction necessary for it to understand the value between ages and gender in a rudimentary fashion albeit. Next, a machine must address the issue of red circles in the blue area and blue in the red, because it wants to avoid false positives or false negatives for the resulting number of female and male in the graph. This relationship shows that the number of false positives rises as the number of true positives increases or more aptly named a receiver operating characteristic, or ROC curve. This curve plots the false positive rate against the true positive rate against the prediction effectiveness. Some graphs will only tolerate on percent of false positives, which will yield 90% correct detections, or true positives and thus means an AI must be fed a lot of data for it to be accurate. To work beyond this researcher, add even more dimensions-for example, since we can only visualize two dimensions on paper, we need to use a technique similar to casting a shadow to properly project multiple dimensions (i.e., a sphere casts part of a circle against a wall or the floor, which is a two-dimensional plane and is flat. A high dimensional or discriminatory AI with a bountiful number of classifications will only serve to distinguish the male and female spectrum and can be plotted against a two-dimensional plane showing the effectiveness of their model with a high degree of true positives. Generalization is another form of incorporating more “true positives”, because a machine that generalizes means it doesn’t need to be updated on a daily basis and instead of having to memorize a set of specific malware file signatures, ML can learn without being fed a new data set every day; as a result, can look at the broader picture – the high-level traits – to decide if a file is malicious. Looking at high-level traits and awarding that machine based on those true positives that are later reviewed and updated can prove to be an effective and accurate ML model, but since it generalizes and in spite of its advantages in doing so, ML is not perfect and is the reason why Crowdstrike’s antivirus engine provides a confidence score, which can assist users in determining if a file is likely to be a false positive/negative. This is also the reason why ML is such an effective tool against advanced unknown malware – because it generalizes it can provide a confidence score on a scan and allow users the chance to investigate data that seems suspicious. 
Machine Learning technologies like recommendation algorithms shown in Netflix, image analysis and object detection, fraud detection, automatic helplines or chatbots, self-driving cars, and medical imaging diagnostics are such technologies that vary in category, either supervised, unsupervised, or reinforced, their effectiveness depends on deep classification models that expand its dimensional capabilities. Going back to image detection a sphere can be classified based on the shadow it gives off, but many different objects are spherical in and of themselves but may not necessarily be called a sphere like a ball, so context may matter in this case and an image detection algorithm thus needs to understand the context with which it is scanning. It’s these classification models that end up feeding into an algorithm’s accuracy, but whether or not it is effective is all determined by its confidence levels, which should be made clear, so users do not take AI at face value for their ingenuity. The limitations of ML in detecting malware are that feeding disinformation to AI can skew its results and this type of AI poisoning attack can affect the accuracy or effectiveness of ML models in general. Since most ML malware engines run on known signatures and generalize based on high-level traits, then one can assume that some of those high-level traits can also begin to incorporate some false positives. Intrusions that do not use malware is another data point that machine learning cannot possibly classify. Changes in user behavior require an invasion of privacy that most will seem as gross and unwarranted and thus with the expanded use of AI mitigating malware-based attacks, so too must socially engineering rise on the other end. 
According to CrowdStrike in the Whitepaper all an attacker needs is 500 unique malware files to achieve 99+ percent certainty that at least one of those files will make into the victim’s machine if it is protected by a machine learning algorithm. The recommended solution for compensating on the limitations of ML in malware detection is to treat ML as part of a comprehensive solution and not an end all be all. Techniques like exploit prevention and behavioral analysis are an approach to detecting indicators of attack (IOAs). These IOAs are determined by analyzing the behavior of events and actions to detect the attacker’s intent, regardless of the malware or exploit used in the attack. Furthermore, advanced endpoint detection and response capabilities are must because it provides the organization with the visibility one needs to see what is happening on the network and what adversaries are doing and stop them before they can cause serious damage. EDR can also be used beyond that as a platform to launch threat hunting which is vital to stopping highly sophisticated attacks orchestrated by advanced adversaries. Besides detecting sophisticated malware AI and ML can improve AI to analyze phishing campaigns and integrate with Security information and event management to provide all of this data at a glance. In my opinion AI is going to be necessary to automating analysis of the QUIC protocol. As networks become faster and protocols are capable of transferring data simultaneously the need for automation will need to change alongside that; so nevertheless, all components of cybersecurity will be assisted by AI in the future, because the level of processing required to mitigate and neutralize threats are only going to rise as innovation streams in. 
References: 
Brown, S. (2021, April 21). Machine Learning, explained. MIT Sloan. Retrieved August 2, 2022, from https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained#:~:text=In%20some%20cases%2C%20machine%20learning,do%20it%2C%E2%80%9D%20he%20said. 
Tang, G. (2021, January). Formal analysis of Quic handshake protocol using symbolic model checking. Retrieved August 2, 2022, from https://www.researchgate.net/publication/348799241_Formal_Analysis_of_QUIC_Handshake_Protocol_Using_Symbolic_Model_Checking 
The Rise of Machine Learning in Cybersecurity. Crowdstrike Whitepaper Machine Learning. (n.d.). Retrieved from https://go.crowdstrike.com/rs/281-OBQ-266/images/WhitepaperMachineLearning.pdf 
The risks of artificial intelligence to security and ... - Rand Corporation. (n.d.). Retrieved August 2, 2022, from https://www.rand.org/content/dam/rand/pubs/perspectives/PE200/PE237/RAND_PE237.pdf 
Neural Networks
1.	Algorithms and models are a vital part of ML. And, while it is common to see these terms used interchangeably, they are not the same (Singh, 2020). In ML, an algorithm is a process that is applied to large datasets to identify patterns and learn from the provided raw data. The end product of this processing and learning is a mode. Essentially, the model is the representation of what was learned by the ML algorithm. From here, the model can then be applied to similar circumstances in the future to make associations, predictions, and recommendations. 
a.	Neural networks are inspired by how neurons operate and interact in the human brain. And, like human neurons, the neural networks allow machines to learn, recognize patterns, and produce outcomes. Neural networks are already behind may decisions about your life. For example, banks use it to evaluate a loan application and also to detect and prevent fraud. Another example is the Pap test for cervical cancer screening, which is challenging to recognize, since this cancer occurs without symptoms. Here, neural networks are used to see the image of cells under a microscope and decide whether there’s a risk of cancer. In addition, Amazon’s Alexa uses neural networks to understand what song you ask her to play and sound more human-like. Lastly, Facebook uses neural networks to suggest photo tags. 
2.	Artificial Neural Nets:
a.	Non-linear models motivated by the physiological architecture of the nervous system.
b.	Cascade simple non-linear computations that when aggregated, implement a robust and complex non-linear function. 
c.	Can approximate any non-linear function, making them a powerful class of computational models.
d.	More efficient with multiple layers at approximating arbitrary functions than other methods. 
3.	Neural Networks:
a.	Input layer
b.	Hidden layer 1
c.	Hidden layer 2
d.	Output layer
e.	Inputs:
i.	X1, x2, x3, xx, w1, w2, w3, w4
f.	Sum function
g.	Activation function
h.	Output
i.	The architecture is the number of layers the neural network has and the design of its edges.
 
4.	Training for one example involves:
a.	Measuring the error between the label of the example and the prediction.
i.	This error is backpropagated to identify the weights contributing to the error and changing them by a small measure in the direction away from the error along the gradient in a method called gradient descent. 
ii.	This is all done with mathematics, algebra, and calculus that is implemented in computer code. 
5.	The more common availability of lots of data enables better training and higher performing models to be trained. 
Model as Detector
1.	Effectively detecting malicious software and files is an increasingly difficult task for organizations and individual users. One of the critical reasons for this is that the sophistication of today’s malware can easily circumvent traditional anti-malware solutions, cause millions of dollars in damage to organizations, and leave individual users paralyzed for weeks. Furthermore, due to the heavy reliance that traditional anti-malware solutions have on static signatures, they are usually inadequate at properly classifying unique malware samples that have never been seen before. Fortunately, the significant development in ML and neural networks research has allowed it to be applied to things beyond its initial image and sound recognition capabilities. So, how can you use neural networks to train and classify whether a file that has never been seen before is benign or malicious?
2.	Models Trained to Detect Malware: Recap
a.	Are trained with labeled examples from which a long bit vector is extracted as features. 
b.	Use back propagation and gradient descent to tune its parameters.
c.	Are tested on the testing subset of examples by running the examples through it and hiding the labels, then comparing its prediction to the label and counting the errors. 
d.	Can be deployed if the accuracy makes the developer confident enough.
e.	Once deployed, they receive a portable executable (PE), extract its features, and then pass through the detector, and the detector’s prediction gives us its label. 
Model Attack
1.	If you have ever had the opportunity to experience a self-driving car or have even watched an online video of one, it is nothing short of seeming magical. Their ability to drive on roads, change lanes, and parallel park is all unbelievable. Moreover, there is no question that years of hard work of numerous engineers, mathematicians, scientists, and others have made today’s self-driving vehicles more capable and powerful than ever before. However, many believe that we are still a long way from entirely self-driving vehicles despite the incredible recent advancements. Along with the impressive demo videos of self-driving cars online, there are also several documented incidents where today’s self-driving vehicles miscalculated a situation and subsequently made the wrong decision. Some examples of these include snowflakes, tree shadows, and bridges confusing cars’ sensors and being misinterpreted as obstacles. Additionally, slight alterations of traffic signs and the generation of certain sound waves have also disoriented cars (Coren, 2018). Unfortunately, the above examples are not limited to self-driving cars but are rather a common challenge for various AI.ML systems. Furthermore, in AI/ML terms, this activity is referred to as adversarial perturbation, a change that fools an ML Systems into making a mistake or changing its behavior (Doctorow, 2017).
2.	Problems in Malware Detector Model:
a.	Incorrect prediction:
i.	False negative: A PE with invasive malware that could be misclassified as benign.
ii.	False positive: A PE that has no malware could be classified as malicious.
b.	Emergence of new malware that was not represented in training examples.
c.	Fundamental vulnerability in machine learning detectors:
i.	Our inability to train a model on every possible example it could see.
ii.	Mathematics of the model learning in terms of how the classification boundary that the model defines between malicious and benign is identified. 
3.	Adversarial Perturbation Attack:
a.	The attacker needs to thwart the detector to execute the malware and thus they look for corner cases called adversarial perturbations.
b.	Input Feature extraction  Not malware/ Malware
Bias in AI
“It’s far too easy to blame ‘the algorithm’ when something goes wrong, but ultimately, humans must bear the responsibility for these decisions, and they must understand where the potential pitfalls lie.” – David COX, IBM Director, MIT-IBM Watson AI Lab
The main underlying components of AI are the algorithm itself, the data, and the team that comes together to create AI. Bias is less likely to be a result of the algorithm itself, but the data and the team are the highest contributors to bias in AI. When gathering data, you need to know whether or not the data you are using is already biased and are there gaps in your data? How are you preparing your data? What information is relevant? And finally, are you testing your biases with the same data? All of these questions are integral to understanding the parts of data and how data can introduce bias into an algorithm. Thus, I believe data is the biggest indicator for bias and yet it’s still very crucial that the team working on the AI is aware of their own biases when testing and cleaning data. The most difficult component to AI to address in terms of bias is the team itself. The team itself must first set the parameters and goals of the AI, what an acceptable error rate is and if they wish to incorporate more false positives or false negatives. This brings us to a philosophical question of whether or not humans are advanced enough to understand their biases and account for those biases within their algorithm. These are the very questions I would ask a cybersecurity vendor who utilizes AI/ML – in order to understand the goals and how the AI fits within the organization you must be able to understand what the goals of that AI is, what data they incorporate, and who the team is. If a model incorporates more false positives then it’s clear that the AI may be fed junk data or is biased to the point that it does not fulfil the organization’s goals in protecting their data. 
In order to reduce implicit and explicit biases from AI/ML most companies tend to incorporate more technologies who target that single goal like: FairML or Arthur AI. On top of this reevaluating the AI process and making it more inclusive can spread the governorship of AI. Finally, society needs to account for biases and build AI for the world we want to live in. If we are not thoughtful about bias we will simply end up automating it. Without questioning the algorithm ask how it was developed, tested, and how they control for bias, both implicit and explicit. The very fact that AI depends on the availability of large quantities of data is a privacy concern for millions of Americans today and it ought to be questioned as well. The very fact we farm data as if it belongs to us and not the individual contributor of that data is already a bias that can be skewed in favor of bad actors with attacks like AI poisoning becoming a thing. It’s definitely a question to consider, but one that requires cybersecurity to mitigate that risk. 
Reference:
A.I. For Anyone. (2020, December 1). Bias in AI | An overview of algorithmic bias in artificial intelligence. Retrieved from YouTube URL: https://www.youtube.com/watch?v=Qr7d01uz8YM
Improvement Methods
1.	As you have reviewed earlier in the course, organizations continue to adopt AI and ML rapidly to solve some of today’s most pressing and complex challenges. AN excellent example of this was during the COVID-19 pandemic when AI provided the much-needed support to determine the most at-risk populations and the most effective way to allocate scarce resources (WSJ Podcasts, 2020). At this point, there is no question that the increased adoption of AI by organizations and individual users will result in cybercriminals exploiting it in various ways. One relevant example of an open-source tool leveraging AI/ML to automate penetration testing is called Deep Exploit, which is built on top of the robust Metasploit framework that is popular among penetration testers and cybercriminals. Deep Exploit features include the ability to self-identify open ports and vulnerabilities on the target system before executing the most efficient exploit (Takaesu, n.d.). As a cybersecurity professional, this is a predicament if immeasurable proportions.
2.	“Rather than fearing or ignoring cyber-attacks, do ensure your cyber resilience to them.” – Stephane Nappo
a.	Recognizing that attackers will use AI to magnify the scope and scale of their attacks, evade detection, and compromise the integrity of AI systems, what can you do? How can you design your AI models to detect malware and other types of intrusions more resiliently? 
3.	Generate Adversarial Examples:
a.	Q: What can be done ahead of time in retaliation to an attacker retaliating to a machine learning malware detector by perturbing their malware?
b.	A: Anticipate the second retaliation and add the examples to the training set to make the models’ boundaries move around them with sensitive precision. 
4.	Sleipnir: Adversarial Model Learning
a.	Step 1: Splits the training data into malicious examples and benign ones.
b.	Step 2: Trains with the malicious examples. 
c.	Step 3: Intervenes and searches for examples that are adversarial perturbations. 
d.	Step 4: Adds adversarial perturbations to the original training set.
e.	Step 5: Trains with the extra potentially malicious training examples and benign ones. 
f.	Results in a model as accurate as the original one; however, it may have some false negatives and false positives. 
Evolution
1.	An ecosystem consists of various living and nonliving things in a given area interacting with each other. The cyber ecosystem similarly includes a variety of participants digitally linked to each other. Within this cyber ecosystem, multiple attackers and defenders continuously strategize to outmaneuver each other to advance their objectives. There are ML algorithms that are inspired by the natural evolution process called evolutionary algorithms. These algorithms are ideal for complex problems where there are many iterations and possibilities. Evolutionary algorithms attempt to solve these difficult use cases by taking inspiration from nature and evaluating the dataset to pick the fittest sample that is likely to survive. Moreover, there is also a coevolutionary algorithm where too evolutionary algorithms (i.e., species or groups) compete, such as the attackers and defenders in the cyber ecosystem. 
2.	Fitter threats and defenses are replicated and adapted, and some of the replications go on to replace older strategies. 
3.	Evolved to Defend:
a.	Chameleons:
i.	Learned to change color to camouflage themselves.
b.	Plants:
i.	Evolved to grow thorns and spikes and to adopt a bad tasting or toxic composition.
c.	Skunks:
i.	Evolved to emit bad smells, and they signal with their distinctive white stripe. 
4.	Evolved to Attack:
a.	Cheetahs:
i.	Evolved to run very fast.
b.	Sit-and-wait predators:
i.	Yellow spider traps bees that visit its yellow flower. 
c.	Gazelles:
i.	Evolved horns to fight one another for dominance because of sexual selection. 
5.	The class of machine learning algorithms that loosely follow the mechanisms of natural evolution are called evolutionary algorithms. 
6.	Strategy Adaptation through Coevolution:
 
How can Humans Thrive in the AI Age?
“Artificial Intelligence isn’t an imagined future. It’s right here, right now. So, what are the perils of society’s rapid pivot to automation? How do we avoid displacement and dehumanization? And, most pressing, how do we find meaning in a world driven by algorithms?” – Rich Roll
There is no doubt that AI and automation will affect almost every industry and profession in some capacity. And while some of the key benefits for the organization include increased efficiencies, performance, and profitability, there is unsurprising concern among today’s workforce about their livelihoods. As a result, a serious question in the minds of today’s workers is not only how to protect their futures, but also how they can thrive in the age of AI and the related automation. A conventional belief is that to compete in the world of AI, you need to become computer-like. It starts with pursuing studies and professions in science, technology, engineering, and mathematics (STEM). Furthermore, it includes becoming hyper-efficient and productive. However, an alternative thesis contradicts the conventional view and suggests that humans are ill-equipped to compete in the areas where computers excel. Therefore, in the age of AI and automation, it proposes that humans are better off cultivating areas that make them unique and where computers are known to be inadequate. The alternative thesis highlighted in the former has been expressed by Kevin Roose, a bestselling author and award-winning columnist for the New York Times, in his latest book, Futureproof: 9 Rules for Humans in the Age of Automation. In the book, Roose makes the case that “in order to survive this wave of AI and automation, we need to become more human.” (Roose, n.d.). 
How can Humans Thrive in the AI Age?
Kevin Roose’s prescription against workforce automation is essentially the antithesis to the growing culture that a machine is something to compete against and instead of training to be more like machines humans need to work on their humanity. AI has definitely taken my workplace by storm, and it seems AI is used primarily to automate menial tasks and retain humanity. My previous workplace on the other hand did not use AI effectively. Sprint utilized natural language models to determine their caller’s moods including their agents and if you showed up more than twice against the AI you would be reprimanded for your tone. This is not what AI should be used for and nevertheless, the project died once T-Mobile took over and was forgotten about until recently when I heard that it had been revived by T-Mobile and now seems to target key words that can be expanded upon in open dialogue without having to go after the representative for the words they evoked. As a neuroscience student I actually took quite a few psychology classes because I was going for Neuropsychology. A blend between the neurosciences and psychology and looking back I’m glad I never entered the field. I’m happy I didn’t throw my life away towards a field that was rapidly becoming automated and less personal. I’m glad my bosses weren’t litigation experts and corrupt marketing agents because I see that the advent of artificial neural nets (this was touched on in cognitive psychology and neurobiology) may have been the cause of expanding bureaucracy and hostile workplaces. The human element is essential to artificial intelligence because it can cause displacement and dehumanization – so how do we find meaning in world driven by algorithms? The answer is to become more human and strive to work with AI instead of against it. Being human also doesn’t mean we can just be complacent, and rest assured in our humanity – it also means we need to become aware of our biases, understand them, question them, and work with them to work towards a common solution that benefits everyone. 
What surprised me the most while listening to and learning Roose’s take on AI and how humans can secure their future is the fact that there are three categories that we can attach value to. Anything surprising, social, or scarce is automatically going to be valuable and can only be attributed to of human “effort”. Attaching value to effort and what humans can do better than AI is human. AI doesn’t do well in the face of uncertainty, AI is not very social, and scarce jobs in general cannot be automated, because those jobs typically require you to be in environments that are either dangerous or require quick decision-making skills. It’s surprising to me that making the internet and objects on the internet scarce can be attributed to an NFT and essentially make a dancing monkey gif worth hundreds of thousands of dollars is the most surprising aspect of this, because generally making NFTs hasn’t become automated yet. It requires a painstaking amount of effort to create the blockchain and brand that blockchain for it to be distributed across the world as well is quintessentially economics in a cyber realm. My perspective of the future of the workforce and AI has shown that the things that spark our imagination and make us happy as humans will come hand in hand when AI takes over boring spreadsheets that show how much the company has made in the last quarter. It will begin to change how the workplace is viewed and I believe that we are heading towards a world of abundance and pleasurable work in the advent of AI. At the end of the podcast Rich Roll begs the question where young people should direct themselves in skills and I think this is heavily important, because he gives an honest answer about what young people should value in general. Listening to Roose talk about values such as turning off your phone and reading books and generally talking about learning how to think instead of worrying on your next exam and overexerting yourself to the point you have no rest is crucial and tantamount to becoming a well-rounded individual and leads credence to what he said before and that is to become emotionally intelligent and learn how to speak with other people. The advent of AI shouldn’t tear us apart we need to come together in order to survive AI. 
References: 
YouTube. (2021). Can Humanity Survive Ai? | Rich Roll Podcast. YouTube. Retrieved August 2, 2022, from https://www.youtube.com/watch?v=8x0FBiSDVcU. 
Module Summary
1.	“I think what makes AI different from other technologies is that it’s going to bring humans and machines closer together. AI is sometimes incorrectly framed as machines replacing humans. It’s not about machines replacing humans, but machines augmenting humans. Humans and machines have different relative strengths and weaknesses, and it’s about the combination of these two that will allow human intents and business process to scale 10x, 100x, and beyond that in the coming years.” — Robin Bordoli, Chief Executive Officer, Figure Eight
a.	In this module, you learned that AIis a broad discipline that focuses on creating intelligent and autonomous systems that can mimic the human brain. Furthermore, under the overarching AI category, some sub-categories, such as ML, concentrate on parsing large datasets to learn and develop models that enable informed future predictive output or decisions.
b.	In the context of cybersecurity, you learned how ML could be trained to help identify whether a file is benign or malicious. However, just like with most pieces of technology, you discovered how attackers could also exploit the gaps in the malware detection ML model. Lastly, you learned how attackers could leverage ML technology to advance their illicit objectives.
Bibliography
“5 Mind-Blowing Ways Facebook Uses Machine Learning.” GeeksforGeeks. 2 Nov. 2019. https://www.geeksforgeeks.org/5-mind-blowing-ways-facebook-uses-machine-learning/. 
A.I. For Anyone. “Bias in AI | An Overview of Algorithmic Bias in Artificial Intelligence.” YouTube video, 52:16. 1 Dec. 2020. https://www.youtube.com/watch?v=Qr7d01uz8YM.
A.I. For Anyone. “Learn AI in 60 Minutes | An Introduction to Artificial Intelligence for Students and Educators.” YouTube video, 1:00:45. n.d. https://www.youtube.com/watch?v=J8isqIBTvYw&t=444s.
A.i.4. “Offense: Hackers Perspective on AI.” YouTube video, 45:53. 14 May. 2020. https://www.youtube.com/watch?v=2ZwxHHHczEA.
Ai in 60 Minutes w/ A.I. for Anyone & Mark Cuban Foundation.” Google Forms. n.d. https://docs.google.com/forms/d/e/1FAIpQLSdWKzq_BRwyma67ypkgEDFA8p36rXN7uIlsA91wOkB4p8OTaA/viewform. 
“AI: 15 Key Moments in the Story of Artificial Intelligence.” BBC Teach. n.d. https://www.bbc.co.uk/teach/ai-15-key-moments-in-the-story-of-artificial-intelligence/zh77cqt. 
“Alternative Investments.” Investopedia. n.d. https://www.investopedia.com/alternative-investments-4427781. 
Anyoha, Rockwell. “The History of Artificial Intelligence.” SITN. 28 Aug. 2017. https://sitn.hms.harvard.edu/flash/2017/history-artificial-intelligence/. 
Black Hat USA 2018. n.d. https://www.blackhat.com/us-18/arsenal/schedule/index.html#deep-exploit-11908. 
CNBC. “Can Facebook and Google Detect and Stop Deepfakes?” YouTube video, 12:56. 26 Sept. 2019. https://www.youtube.com/watch?v=4YpoYvhVmDw.
Coren, Michael J. “All the Things That Still Baffle Self-Driving Cars, Starting with Seagulls.” Quartz. n.d. https://qz.com/1397504/all-the-things-that-still-baffle-self-driving-cars-starting-with-seagulls/. 
Devi, M. Anousouya, S. Ravi, J. Vaishnavi, and S. Punitha. “Classification of Cervical Cancer Using Artificial Neural Networks.” Elsevier. 22 Aug. 2016. https://www.sciencedirect.com/science/article/pii/S187705091631170X. 
Dixon, William. “What Is Adversarial Artificial Intelligence and Why Does it Matter?” World Economic Forum. 21 Nov. 2018. https://www.weforum.org/agenda/2018/11/what-is-adversarial-artificial-intelligence-is-and-why-does-it-matter/. 
Doctorow, Cory. “‘Adversarial Perturbations’ Reliably Trick AIs about What Kind of Road-Sign They're Seeing.” BoingBoing. 7 Aug. 2017. https://boingboing.net/2017/08/07/nam-shub-of-enki.html. 
Ezekiel, Sarah & Google Creative Lab. “Look to Speak.” Experiments with Google. Dec. 2020. https://experiments.withgoogle.com/looktospeak.  
“G Data Threat Analysis 2020: Cyber Attacks Every Second.” G DATA. n.d. https://www.gdatasoftware.com/news/2021/02/36663-g-data-threat-analysis-2020-cyber-attacks-every-second. 
Gaylor, Brett. Discriminator. n.d. https://www.discriminator.film/. 
Google. “Meet Your Google Assistant, Your Own Personal Google.” YouTube video, 1:35. 4 Oct. 2016. https://www.youtube.com/watch?v=FPfQMVf4vwQ. 
Google Research, Google Creative Lab, and YouTube Music. “FreddieMeter.” Experiments with Google. Nov. 2019. https://experiments.withgoogle.com/freddiemeter. 
Grammarly. “How Does Grammarly Work?” YouTube video, 0:53. 27 Feb. 2019. https://www.youtube.com/watch?v=_yj-hjzql3I. 
“How A.I. is Filling in Coronavirus Testing Gaps.” WSJ Podcasts. 20 Apr. 2020. https://www.wsj.com/podcasts/tech-news-briefing/how-ai-is-filling-in-coronavirus-testing-gaps/bc030bfc-958a-4777-9420-3e9e22e32b3c. 
“Interplay Mode Making Videos Interactive with AI.” Google. n.d. https://experiments.withgoogle.com/interplay-mode/view/.
Jovanović, Bojan. “55 Fascinating AI Statistics and Trends for 2021.” DataProt. 16 Mar. 2021. https://dataprot.net/statistics/ai-statistics/#:~:text=Key%20AI%20statistics,billion%20a%20year%20by%202025. 
Kevin Roose. n.d. https://www.kevinroose.com/futureproof. 
Lobe. n.d. https://www.lobe.ai/. 
Mann, Yotam. “AI Duet.” Experiments with Google. May. 2017. https://experiments.withgoogle.com/ai-duet. 
Mercedes-Benz. “EQS with Unique MBUX Hyperscreen: The Big In-Car Cinema.” YouTube video, 1:10. 7 Jan. 2021. https://www.youtube.com/watch?v=Tv-Jw_aZx0M. 
Mishkin, Pamela, Russell Goldenberg, Jan Diehm, and Maria Scherlies. The Pudding. n.d. https://pudding.cool/2021/03/love-and-ai/. 
UBTECH Robotics. “ADIBOT: The UV-C Disinfection Robotic System.” YouTube video, 3:16. 9 Feb. 2021. https://www.youtube.com/watch?v=hNjqwu12U1c. 
“Quick Draw!” Google. n.d. https://quickdraw.withgoogle.com/. 
Roose, Kevin. “The Value of Your Humanity in an Automated Future.” TED, October 2020. https://www.ted.com/talks/kevin_roose_the_value_of_your_humanity_in_an_automated_future. 
Samsung. “[CES 2021] Next Generation Robotics | Samsung.” YouTube video, 2:20. 11 Jan. 2021. https://www.youtube.com/watch?v=3H6g19EhXQQ. 
Seetharaman, Krishna. “Financial Applications of Neural Networks.” Aspire Systems. 22 Feb. 2018. https://blog.aspiresys.com/banking-and-finance/financial-applications-neural-networks/. 
Singh, Amarpreet. “Difference Between ML Algorithm and Model.” Brandlitic. 17 Jun. 2020. https://medium.com/brandlitic/difference-between-ml-algorithm-and-model-801a798a6dc0. 
Sullivan, Mark. “Amazon Is Injecting Alexa with More Artificial Intelligence than Ever.” FastCompany. 26 Sept. 2019. https://www.fastcompany.com/90409535/little-by-little-amazon-is-giving-alexa-more-ai-smarts. 
Teachable Machine. n.d. https://teachablemachine.withgoogle.com/. 
“The Emergence of Offensive AI.” PDF file. Forrester. Feb. 2020. https://www.darktrace.com/en/resources/research-forrester-offensive-ai.pdf.
“The Rise of Machine Learning in Cybersecurity.” PDF file. Crowdstrike. n.d. https://go.crowdstrike.com/rs/281-OBQ-266/images/WhitepaperMachineLearning.pdf.
Theohary, Catherine A. “Information Warfare: Issues for Congress.” PDF file. Congressional Research Service. 5 Mar. 2018. https://sgp.fas.org/crs/natsec/R45142.pdf.
UBTECH Robotics. “ADIBOT: The UV-C Disinfection Robotic System.” YouTube video, 3:16. 9 Feb. 2021. https://www.youtube.com/watch?v=hNjqwu12U1c. 
WeAreNetflix. “Netflix Research: Machine Learning Platform.” YouTube video, 3:00. 3 Sept. 2018. https://www.youtube.com/watch?v=VvTYuQPINec. 
“What Is Artificial Intelligence?” NetApp. n.d. https://www.netapp.com/artificial-intelligence/what-is-artificial-intelligence/. 
Yesterday, Today, Tomorrow. n.d. https://yesterday.nfb.ca/.
YSL Beauty. “Rouge Sur Mesure Personal Lipstick Shade Creator | YSL BEAUTY.” YouTube video, 1:07. 20 Jan. 2021. https://www.youtube.com/watch?v=00nXR_WUpKY.

